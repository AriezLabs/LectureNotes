# 19-10-2018

<!--TOC-->

## Finding literature

Some review journals: 

* ACM Computing Reviews
* ACM Guide to Computing Literature
* Zentralblatt fuer Mathematik
* SIAM Review

They publish short summaries and critical evaluations of research in a particular area. Process takes a while - they might lag behind by years and only a small number of papers make their way to them.

*Bibliographies* are quite valuable: complete surveys on a particular subject and lots of references. 

The *Science Citation Index*: "Normal" citations refer to older papers. This thing reverses this: i.e. says "this paper is cited by..."; allowing you to search both towards the past and the future relative to the date of publication of some paper

* This can also be used to rate the importance of journals and papers

Today finding information is simple - Google (esp. [Google Scholar][2]). We also have [libraries][1]; e.g. [PubMed][3] is the main source for biomedical stuff

Finding a paper: three possibilities

1. [*Unibibliothek*][4]
2. Trying to find author's homepage on the web
3. Mailing author asking for the paper (address person according to their website - PhD students can be addressed as Mr./Mrs.)
4. Ask advisors, colleagues, ...

## Measuring quality

of journals, papers, scholars. We assign a number representing quality in order to obtain a total order. We have various scores:

#### Journal Impact Factor

* Measures the average rate at which a journal is cited in scientific literature.
* How is this calculated? We define 
    * $S :=$ *number of papers published in the journal in the past two years* 
    * $R :=$ *number of publications referring to the journal in the past year* 
    * $\frac RS$ gives the JIF.
* There are criticisms: 
    * There has been obvious cheating
    * Single papers with lots of citations might result in massive changes in JIF
    * JIF differs between subjects because the relative number of citations that papers obtain during the first two years after publication varies
        * CS and math JIFs usually hover around 10 while good journals in the biosciences achiever around 30

#### h-Index and similar indices

* given by $h$ where, intuitively, $h$ is the side length of the largest square that fits under the curve given by the number of citations on the y-axis and the publications sorted in descending order on the x-axis

<img src="./h-index.png">

* **e-index** is the area on top of the square
* **g-index**, **r-index**, ... also exist

#### General problems with indices

* Rely on the basic assumption that more citations = better paper
    * Some publications are cited as examples of a *bad* paper
    * Small or new fields are disadvantaged
* Where do you count citations? Do books and conferences count?
* How do you treat self-citations?
* How do you automatically distinguish authors with the same name? (especially ones that work in the same field?)
    * *ORCID*: a persistent digital identifier for researchers
* If value is attached to poor measures this may lead to poor practices
    * e.g. publishing multiple papers when one would suffice - more publications, more (self-)citations

[1]: https://dl.acm.org/dl.cfm
[2]: https://scholar.google.com
[3]: https://www.ncbi.nlm.nih.gov/pubmed/
[4]: https://www.ubs.sbg.ac.at/dbis/
